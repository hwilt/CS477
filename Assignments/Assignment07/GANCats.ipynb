{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2473b574",
   "metadata": {},
   "source": [
    "# This Cat Doesn't Exist\n",
    "## Generative Adversarial Networks Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699eef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f387e",
   "metadata": {},
   "source": [
    "# Dataset Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### RUN THIS CODE ONLY IF YOU ARE ON GOOGLE COLAB! ########\n",
    "###### OTHERWISE, YOU SHOULD DELETE THIS CELL!!       ########\n",
    "## !wget https://github.com/ursinus-cs477-f2023/HW7_GAN/archive/refs/heads/main.zip\n",
    "## !unzip main.zip\n",
    "## !mv HW7_GAN-main/cats .\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(XGen):\n",
    "    \"\"\"\n",
    "    Plot a set of examples from the dataset/generator on a square grid\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    XGen: torch.tensor(n_examples, 3, dim, dim)\n",
    "        A batch of examples to plot\n",
    "    \"\"\"\n",
    "    k = int(np.sqrt(XGen.shape[0]))\n",
    "    for i in range(XGen.shape[0]):\n",
    "        plt.subplot(k, k, i+1)\n",
    "        Xi = XGen[i, :, :, :].detach().cpu().numpy()\n",
    "        Xi = np.moveaxis(Xi, 0, 2)\n",
    "        Xi[Xi < 0] = 0\n",
    "        Xi[Xi > 1] = 1\n",
    "        plt.imshow(Xi)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "class CatData(Dataset):\n",
    "    def __init__(self, foldername, imgres=64, train=True):\n",
    "        self.images = glob.glob(\"{}/*\".format(foldername))\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(imgres),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx])\n",
    "        img = self.preprocess(img)\n",
    "        if torch.max(img) > 1:\n",
    "            img /= 255\n",
    "        return img\n",
    "    \n",
    "traindata = CatData(\"cats\")\n",
    "\n",
    "samples = DataLoader(traindata, batch_size=16, shuffle=True)\n",
    "plot_samples(next(iter(samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1244fa8c",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, depth=4, dim_latent=64, dim_img=64, in_channels=3, start_channels=64):\n",
    "        \"\"\"\n",
    "        depth: int\n",
    "            How many convolutional layers there are in the encoder/decoder\n",
    "        dim_latent: int\n",
    "            Dimension of the flattened latent space\n",
    "        dim_digit: int\n",
    "            Width/height of input image\n",
    "        in_channels: int\n",
    "            Number of channels of input image\n",
    "        start_channels: int\n",
    "            Number of channels out of the first convolutional layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.dim_latent = dim_latent\n",
    "        self.dim_img = dim_img\n",
    "        self.in_channels = in_channels\n",
    "        self.start_channels = start_channels\n",
    "        \n",
    "        layers = []\n",
    "        channels = start_channels\n",
    "        for _ in range(depth):\n",
    "            layers.append(nn.Conv2d(in_channels, channels, kernel_size=4, stride=2, padding=1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(channels))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            in_channels = channels\n",
    "            channels *= 2\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(in_channels * (dim_img // (2 ** depth)) ** 2, 1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.conv_layers(X)\n",
    "        X = self.flatten(X)\n",
    "        X = self.linear(X)\n",
    "        return X\n",
    "        \n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, depth=4, dim_latent=64, dim_img=64, in_channels=3, end_channels=64):\n",
    "        \"\"\"\n",
    "        depth: int\n",
    "            How many convolutional layers there are in the encoder/decoder\n",
    "        dim_latent: int\n",
    "            Dimension of the latent space\n",
    "        dim_digit: int\n",
    "            Width/height of input image\n",
    "        in_channels: int\n",
    "            Number of channels of input image\n",
    "        end_channels: int\n",
    "            Number of channels out of the second to last convolutional layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        imgres = dim_img//(2*depth) \n",
    "        in_channels = end_channels*(2*(depth-1)) \n",
    "        shape_latent = (in_channels, imgres, imgres)\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "        nn.Linear(dim_latent, np.prod(shape_latent)),\n",
    "        nn.Unflatten(1, shape_latent),\n",
    "        nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        channels = in_channels\n",
    "        for _ in range(depth):\n",
    "            layers.append(nn.ConvTranspose2d(channels, channels//2, kernel_size=4, stride=2, padding=1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(channels//2))\n",
    "            if channels > 3:\n",
    "                layers.append(nn.LeakyReLU(0.2))\n",
    "            else:\n",
    "                layers.append(nn.Sigmoid())\n",
    "            channels = channels // 2\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.linear(z)\n",
    "        z = self.conv_layers(z)\n",
    "        return z\n",
    "\n",
    "    def sample(self, n_examples, device):\n",
    "        \"\"\"\n",
    "        Sample from the latent space and generate the images\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_examples: int\n",
    "            Number of examples to generate\n",
    "        device: string\n",
    "            Device for model/tensors\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor(n_examples, 3, dim_img, dim_img)\n",
    "            A batch of generated examples\n",
    "        \"\"\"\n",
    "        z = torch.randn(n_examples, dim_latent).to(device)\n",
    "        generated_images = self.forward(z)\n",
    "        return generated_images\n",
    "    \n",
    "\n",
    "## TODO: Test your models here with dummy data before you proceed to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13238d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator(depth=4, dim_latent=64, dim_img=64, in_channels=3, start_channels=64)\n",
    "X = torch.zeros(16, 3, 64, 64)\n",
    "discriminator(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6b05d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(depth=4, dim_latent=64, dim_img=64, in_channels=3, end_channels=64)\n",
    "z = torch.randn(16, 64)\n",
    "XEst = generator(z)\n",
    "plot_samples(XEst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de7454",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e9bfb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "depth = 2\n",
    "dim_latent = 32\n",
    "channels = 32\n",
    "lr = 3e-4\n",
    "\n",
    "n_epochs = 40\n",
    "batch_size = 16\n",
    "\n",
    "## TODO: Fill this in\n",
    "\n",
    "# Generate fake images\n",
    "noise = torch.randn(batch_size, dim_latent, 1, 1)\n",
    "fake_images = generator(noise)\n",
    "\n",
    "# Pass fake images through the discriminator\n",
    "fake_outputs = discriminator(fake_images)\n",
    "\n",
    "# Compute the generator loss\n",
    "generator_loss = F.binary_cross_entropy_with_logits(fake_outputs, torch.ones_like(fake_outputs))\n",
    "\n",
    "# Compute gradients and take a step in the generator optimizer\n",
    "generator_loss.backward()\n",
    "generator_optimizer.step()\n",
    "\n",
    "# Zero out the discriminator's gradients\n",
    "discriminator_optimizer.zero_grad()\n",
    "\n",
    "# Pass real images through the discriminator\n",
    "real_outputs = discriminator(real_images)\n",
    "\n",
    "# Compute the discriminator loss for real images\n",
    "real_loss = F.binary_cross_entropy_with_logits(real_outputs, torch.ones_like(real_outputs))\n",
    "\n",
    "# Pass fake images through the discriminator\n",
    "fake_outputs = discriminator(fake_images.detach())\n",
    "\n",
    "# Compute the discriminator loss for fake images\n",
    "fake_loss = F.binary_cross_entropy_with_logits(fake_outputs, torch.zeros_like(fake_outputs))\n",
    "\n",
    "# Compute the total discriminator loss\n",
    "discriminator_loss = real_loss + fake_loss\n",
    "\n",
    "# Compute gradients and take a step in the discriminator optimizer\n",
    "discriminator_loss.backward()\n",
    "discriminator_optimizer.step()\n",
    "\n",
    "# Output generated examples at the end of each epoch\n",
    "plot_samples(generator.sample(16, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723e110c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
